{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8a7f99",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-04T10:39:44.844757Z",
     "iopub.status.busy": "2025-10-04T10:39:44.844541Z",
     "iopub.status.idle": "2025-10-04T10:40:19.443405Z",
     "shell.execute_reply": "2025-10-04T10:40:19.442667Z"
    },
    "papermill": {
     "duration": 34.603688,
     "end_time": "2025-10-04T10:40:19.444714",
     "exception": false,
     "start_time": "2025-10-04T10:39:44.841026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-04 10:39:44--  https://huggingface.co/MapleF/eva_x/resolve/main/eva_x_base_patch16_merged520k_mim.pt\r\n",
      "Resolving huggingface.co (huggingface.co)... 3.169.137.5, 3.169.137.119, 3.169.137.19, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|3.169.137.5|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/659e6a763232931c9a49ee20/f6e7bd72aa9ec12b574f8bbc89a4f2006ffa2cd5bc8dcdd4866ad4f0aa395a2a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251004%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251004T103945Z&X-Amz-Expires=3600&X-Amz-Signature=4479b3aac7b7bbe9aba24754a151c79f02933fc5dfe7bdf865f50cd240f75e06&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27eva_x_base_patch16_merged520k_mim.pt%3B+filename%3D%22eva_x_base_patch16_merged520k_mim.pt%22%3B&x-id=GetObject&Expires=1759577985&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTU3Nzk4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTllNmE3NjMyMzI5MzFjOWE0OWVlMjAvZjZlN2JkNzJhYTllYzEyYjU3NGY4YmJjODlhNGYyMDA2ZmZhMmNkNWJjOGRjZGQ0ODY2YWQ0ZjBhYTM5NWEyYSoifV19&Signature=HXkXKPF%7E8JTZps81wjV0dzl1M5ieSM2Tik9OjLrznyTbyqxelkSsF0tape4eydpAcCcvmf%7EEinuQmmWM1EpfkRU-BiOzNkM3%7EbKttTGnTeS5Cdg0YJ19w1u6zOlblka%7E%7E67yGIAZ7xqD0upy3xs1x-z5wvw57j-uZ%7EyWbecaW1vBAhnBqVnE3WII%7ER5x3vRSPykF-O2-Ul3TdWHWdv89aHkAFIwXzv5okv5Y1BHlzspAmvbsrYqWaK7cQgLwJ86gA7t9wlH0N14Kc80iHPYjfPgYCVvSykZ9w4AXQQfIc3COOXRp%7Eat8EVSd6BbWg4XD6eDawh%7EO4ZYO%7ErcEJ7562g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\r\n",
      "--2025-10-04 10:39:45--  https://cas-bridge.xethub.hf.co/xet-bridge-us/659e6a763232931c9a49ee20/f6e7bd72aa9ec12b574f8bbc89a4f2006ffa2cd5bc8dcdd4866ad4f0aa395a2a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251004%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251004T103945Z&X-Amz-Expires=3600&X-Amz-Signature=4479b3aac7b7bbe9aba24754a151c79f02933fc5dfe7bdf865f50cd240f75e06&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27eva_x_base_patch16_merged520k_mim.pt%3B+filename%3D%22eva_x_base_patch16_merged520k_mim.pt%22%3B&x-id=GetObject&Expires=1759577985&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTU3Nzk4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTllNmE3NjMyMzI5MzFjOWE0OWVlMjAvZjZlN2JkNzJhYTllYzEyYjU3NGY4YmJjODlhNGYyMDA2ZmZhMmNkNWJjOGRjZGQ0ODY2YWQ0ZjBhYTM5NWEyYSoifV19&Signature=HXkXKPF%7E8JTZps81wjV0dzl1M5ieSM2Tik9OjLrznyTbyqxelkSsF0tape4eydpAcCcvmf%7EEinuQmmWM1EpfkRU-BiOzNkM3%7EbKttTGnTeS5Cdg0YJ19w1u6zOlblka%7E%7E67yGIAZ7xqD0upy3xs1x-z5wvw57j-uZ%7EyWbecaW1vBAhnBqVnE3WII%7ER5x3vRSPykF-O2-Ul3TdWHWdv89aHkAFIwXzv5okv5Y1BHlzspAmvbsrYqWaK7cQgLwJ86gA7t9wlH0N14Kc80iHPYjfPgYCVvSykZ9w4AXQQfIc3COOXRp%7Eat8EVSd6BbWg4XD6eDawh%7EO4ZYO%7ErcEJ7562g__&Key-Pair-Id=K2L8F4GPSG1IFC\r\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.169.55.31, 3.169.55.68, 3.169.55.117, ...\r\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.169.55.31|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1210286919 (1.1G)\r\n",
      "Saving to: ‘eva_x_base_patch16_merged520k_mim.pt’\r\n",
      "\r\n",
      "eva_x_base_patch16_ 100%[===================>]   1.13G  33.9MB/s    in 34s     \r\n",
      "\r\n",
      "2025-10-04 10:40:19 (34.1 MB/s) - ‘eva_x_base_patch16_merged520k_mim.pt’ saved [1210286919/1210286919]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/MapleF/eva_x/resolve/main/eva_x_base_patch16_merged520k_mim.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4554a519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:19.461517Z",
     "iopub.status.busy": "2025-10-04T10:40:19.461261Z",
     "iopub.status.idle": "2025-10-04T10:40:37.171603Z",
     "shell.execute_reply": "2025-10-04T10:40:37.170834Z"
    },
    "papermill": {
     "duration": 17.720209,
     "end_time": "2025-10-04T10:40:37.173190",
     "exception": false,
     "start_time": "2025-10-04T10:40:19.452981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from timm.models.eva import Eva\n",
    "from timm.layers import resample_abs_pos_embed, resample_patch_embed\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torchvision.io as io\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as T\n",
    "# Use the recommended v2 transforms\n",
    "from torchvision.transforms import v2\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c450cafd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:37.188997Z",
     "iopub.status.busy": "2025-10-04T10:40:37.188596Z",
     "iopub.status.idle": "2025-10-04T10:40:37.197960Z",
     "shell.execute_reply": "2025-10-04T10:40:37.197286Z"
    },
    "papermill": {
     "duration": 0.018159,
     "end_time": "2025-10-04T10:40:37.199027",
     "exception": false,
     "start_time": "2025-10-04T10:40:37.180868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(22)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43f3424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:37.215017Z",
     "iopub.status.busy": "2025-10-04T10:40:37.214810Z",
     "iopub.status.idle": "2025-10-04T10:40:37.936814Z",
     "shell.execute_reply": "2025-10-04T10:40:37.935503Z"
    },
    "papermill": {
     "duration": 0.731599,
     "end_time": "2025-10-04T10:40:37.938564",
     "exception": false,
     "start_time": "2025-10-04T10:40:37.206965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1030129473.py:59: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=1, max_height=20, max_width=20, p=0.2),\n",
      "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_cols = [\n",
    "            \"Atelectasis\",\"Cardiomegaly\",\"Consolidation\",\"Edema\",\n",
    "            \"Enlarged Cardiomediastinum\",\"Fracture\",\"Lung Lesion\",\n",
    "            \"Lung Opacity\",\"No Finding\",\"Pleural Effusion\",\"Pleural Other\",\n",
    "            \"Pneumonia\",\"Pneumothorax\",\"Support Devices\",\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"Image_name\"])\n",
    "\n",
    "        # Fast read with OpenCV. It may return None if file missing -> handle.\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        # If grayscale (H, W), convert to 3-channel\n",
    "        if img.ndim == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        elif img.shape[2] == 4:\n",
    "            # if RGBA, convert to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        # OpenCV loads BGR -> convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented[\"image\"]\n",
    "\n",
    "        labels = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n",
    "        return img, labels\n",
    "\n",
    "# choose img_size once\n",
    "img_size = 448\n",
    "size_tuple = (img_size, img_size)   # IMPORTANT: albumentations expects a tuple\n",
    "\n",
    "train_tfms = A.Compose([\n",
    "    A.RandomResizedCrop(size=size_tuple, scale=(0.85, 1.0), ratio=(0.9, 1.1)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=12, p=0.4),\n",
    "    A.CLAHE(p=0.3),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.CoarseDropout(max_holes=1, max_height=20, max_width=20, p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.05, rotate_limit=0, p=0.25),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_tfms = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),   # pass height & width\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23d0a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:37.963060Z",
     "iopub.status.busy": "2025-10-04T10:40:37.962789Z",
     "iopub.status.idle": "2025-10-04T10:40:37.979090Z",
     "shell.execute_reply": "2025-10-04T10:40:37.978194Z"
    },
    "papermill": {
     "duration": 0.029379,
     "end_time": "2025-10-04T10:40:37.980697",
     "exception": false,
     "start_time": "2025-10-04T10:40:37.951318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkpoint_filter_fn(\n",
    "        state_dict,\n",
    "        model,\n",
    "        interpolation='bicubic',\n",
    "        antialias=True,\n",
    "):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    state_dict = state_dict.get('model_ema', state_dict)\n",
    "    state_dict = state_dict.get('model', state_dict)\n",
    "    state_dict = state_dict.get('module', state_dict)\n",
    "    state_dict = state_dict.get('state_dict', state_dict)\n",
    "    # prefix for loading OpenCLIP compatible weights\n",
    "    if 'visual.trunk.pos_embed' in state_dict:\n",
    "        prefix = 'visual.trunk.'\n",
    "    elif 'visual.pos_embed' in state_dict:\n",
    "        prefix = 'visual.'\n",
    "    else:\n",
    "        prefix = ''\n",
    "    mim_weights = prefix + 'mask_token' in state_dict\n",
    "    no_qkv = prefix + 'blocks.0.attn.q_proj.weight' in state_dict\n",
    "\n",
    "    len_prefix = len(prefix)\n",
    "    for k, v in state_dict.items():\n",
    "        if prefix:\n",
    "            if k.startswith(prefix):\n",
    "                k = k[len_prefix:]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if 'rope' in k:\n",
    "            # fixed embedding no need to load buffer from checkpoint\n",
    "            continue\n",
    "\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            _, _, H, W = model.patch_embed.proj.weight.shape\n",
    "            if v.shape[-1] != W or v.shape[-2] != H:\n",
    "                v = resample_patch_embed(\n",
    "                    v,\n",
    "                    (H, W),\n",
    "                    interpolation=interpolation,\n",
    "                    antialias=antialias,\n",
    "                    verbose=True,\n",
    "                )\n",
    "        elif k == 'pos_embed' and v.shape[1] != model.pos_embed.shape[1]:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)\n",
    "            v = resample_abs_pos_embed(\n",
    "                v,\n",
    "                new_size=model.patch_embed.grid_size,\n",
    "                num_prefix_tokens=num_prefix_tokens,\n",
    "                interpolation=interpolation,\n",
    "                antialias=antialias,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        k = k.replace('mlp.ffn_ln', 'mlp.norm')\n",
    "        k = k.replace('attn.inner_attn_ln', 'attn.norm')\n",
    "        k = k.replace('mlp.w12', 'mlp.fc1')\n",
    "        k = k.replace('mlp.w1', 'mlp.fc1_g')\n",
    "        k = k.replace('mlp.w2', 'mlp.fc1_x')\n",
    "        k = k.replace('mlp.w3', 'mlp.fc2')\n",
    "        if no_qkv:\n",
    "            k = k.replace('q_bias', 'q_proj.bias')\n",
    "            k = k.replace('v_bias', 'v_proj.bias')\n",
    "\n",
    "        if mim_weights and k in ('mask_token', 'lm_head.weight', 'lm_head.bias', 'norm.weight', 'norm.bias'):\n",
    "            if k == 'norm.weight' or k == 'norm.bias':\n",
    "                # try moving norm -> fc norm on fine-tune, probably a better starting point than new init\n",
    "                k = k.replace('norm', 'fc_norm')\n",
    "            else:\n",
    "                # skip pretrain mask token & head weights\n",
    "                continue\n",
    "\n",
    "        out_dict[k] = v\n",
    "\n",
    "    return out_dict\n",
    "\n",
    "class EVA_X(Eva):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EVA_X, self).__init__(**kwargs)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x, rot_pos_embed = self._pos_embed(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, rope=rot_pos_embed)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        if self.global_pool:\n",
    "            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n",
    "        x = self.fc_norm(x)\n",
    "        x = self.head_drop(x)\n",
    "        return x if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def eva_x_base_patch16(pretrained=False):\n",
    "    model = EVA_X(\n",
    "        img_size=img_size,\n",
    "        patch_size=16,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        qkv_fused=False,\n",
    "        mlp_ratio=4 * 2 / 3,\n",
    "        swiglu_mlp=True,\n",
    "        scale_mlp=True,\n",
    "        use_rot_pos_emb=True,\n",
    "        ref_feat_shape=(14, 14),  # 224/16\n",
    "    )\n",
    "    eva_ckpt = checkpoint_filter_fn(torch.load(pretrained, map_location='cpu', weights_only=False), \n",
    "                        model)\n",
    "    msg = model.load_state_dict(eva_ckpt, strict=False)\n",
    "    print(msg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3c7c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:37.998789Z",
     "iopub.status.busy": "2025-10-04T10:40:37.998558Z",
     "iopub.status.idle": "2025-10-04T10:40:38.005425Z",
     "shell.execute_reply": "2025-10-04T10:40:38.004598Z"
    },
    "papermill": {
     "duration": 0.017974,
     "end_time": "2025-10-04T10:40:38.007108",
     "exception": false,
     "start_time": "2025-10-04T10:40:37.989134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#evax base\n",
    "class EVAX_Model(nn.Module):\n",
    "    def __init__(self, unfreeze_last_n_blocks=4 ):\n",
    "        super().__init__()\n",
    "        # Load Phikon backbone\n",
    "        eva_x_b_pt = '/kaggle/working/eva_x_base_patch16_merged520k_mim.pt'  # Replace with your checkpoint path\n",
    "        self.backbone = eva_x_base_patch16(pretrained=eva_x_b_pt)\n",
    "        \n",
    "        self.backbone.head=nn.Linear(in_features=768, out_features=14, bias=True)\n",
    "\n",
    "        # # Also unfreeze head and normalisation layers around it\n",
    "        for p in self.backbone.head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "    \n",
    "        #return self.head(backbone_output)\n",
    "        return backbone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811617d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:38.024800Z",
     "iopub.status.busy": "2025-10-04T10:40:38.024555Z",
     "iopub.status.idle": "2025-10-04T10:40:38.117980Z",
     "shell.execute_reply": "2025-10-04T10:40:38.117362Z"
    },
    "papermill": {
     "duration": 0.102565,
     "end_time": "2025-10-04T10:40:38.119035",
     "exception": false,
     "start_time": "2025-10-04T10:40:38.016470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scaler = GradScaler(device=\"cuda\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train One Epoch (with AMP)\n",
    "# -----------------------------\n",
    "# def train_one_epoch(model, loader, optimizer, criterion, ema=None, grad_clip=1.0):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     progress_bar = tqdm(loader, desc=\"[Train]\")\n",
    "#     for imgs, labels in progress_bar:\n",
    "#         imgs, labels = imgs.to(DEVICE), labels.to(DEVICE).float()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         with autocast(device_type=\"cuda\" ):  # mixed precision forward\n",
    "#             outputs = model(imgs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         # scaler.unscale_(optimizer)\n",
    "#         # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        \n",
    "#         if ema is not None:\n",
    "#             ema.update(model)\n",
    "\n",
    "#         running_loss += loss.item() * imgs.size(0)\n",
    "#     return running_loss / len(loader.dataset)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, ema=None, grad_clip=1.0, accum_steps=2, scheduler=None):\n",
    "    \"\"\"\n",
    "    accum_steps: number of mini-batches to accumulate before an optimizer step.\n",
    "    scheduler: optional LR scheduler. If provided, step it once per optimizer.step().\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"[Train]\")\n",
    "    for batch_idx, (imgs, labels) in progress_bar:\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True).float()\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # scale the loss down by accum_steps so gradients match larger batch\n",
    "        loss = loss / accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Do optimizer step once every accum_steps\n",
    "        if (batch_idx + 1) % accum_steps == 0:\n",
    "            # # IMPORTANT: unscale before clipping\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # # clip grads on underlying model (unwrap possible DataParallel)\n",
    "            # torch.nn.utils.clip_grad_norm_(get_model(model).parameters(), max_norm=grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # scheduler should step per optimizer step (if you want)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # EMA update should be done after the actual optimizer step\n",
    "            if ema is not None:\n",
    "                ema.update(model)\n",
    "\n",
    "        # accumulate running loss (multiply back by accum_steps to report per-sample loss)\n",
    "        running_loss += loss.item() * imgs.size(0) * accum_steps\n",
    "\n",
    "    # If total batches is not divisible by accum_steps, we should have stepped for the remainder.\n",
    "    # The above code zeroes grads after each step, but if final batch didn't trigger a step, we need to step now.\n",
    "    remainder = len(loader) % accum_steps\n",
    "    if remainder != 0:\n",
    "        # # final step for remaining gradients\n",
    "        # scaler.unscale_(optimizer)\n",
    "        # torch.nn.utils.clip_grad_norm_(get_model(model).parameters(), max_norm=grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "    avg_loss = running_loss / (len(loader.dataset))\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Validation (macro + per-label AUROC)\n",
    "# -----------------------------\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels, all_outputs = [], []\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=\"[Val]\")\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in progress_bar:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE).float()\n",
    "            with autocast(device_type=\"cuda\" ):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_outputs.append(outputs.sigmoid().cpu().numpy())\n",
    "\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "\n",
    "    per_label_auc = {}\n",
    "    for i, label in enumerate(loader.dataset.label_cols):\n",
    "        try:\n",
    "            per_label_auc[label] = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n",
    "        except ValueError:\n",
    "            per_label_auc[label] = np.nan\n",
    "\n",
    "    try:\n",
    "        #macro_auc = roc_auc_score(all_labels, all_outputs, average=\"macro\", multi_class=\"ovo\")\n",
    "        macro_auc = roc_auc_score(all_labels, all_outputs, average=\"macro\")\n",
    "    except ValueError:\n",
    "        macro_auc = np.nan\n",
    "\n",
    "    return running_loss / len(loader.dataset), macro_auc, per_label_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4215c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:38.134921Z",
     "iopub.status.busy": "2025-10-04T10:40:38.134694Z",
     "iopub.status.idle": "2025-10-04T10:40:38.142350Z",
     "shell.execute_reply": "2025-10-04T10:40:38.141849Z"
    },
    "papermill": {
     "duration": 0.01671,
     "end_time": "2025-10-04T10:40:38.143288",
     "exception": false,
     "start_time": "2025-10-04T10:40:38.126578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # pt = p if target=1, 1-p otherwise\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class WeightedFocalBCELoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weights=None):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(alpha, gamma)\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "        self.alpha = 0.7  # Weight between focal and BCE\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        focal_loss = self.focal(inputs, targets)\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        return self.alpha * focal_loss + (1 - self.alpha) * bce_loss\n",
    "\n",
    "# Calculate better class weights\n",
    "def calculate_effective_weights(df, label_cols, beta=0.999):\n",
    "    \"\"\"Calculate more robust class weights using effective number of samples\"\"\"\n",
    "    n = len(df)\n",
    "    weights = []\n",
    "    for col in label_cols:\n",
    "        pos_count = df[col].sum()\n",
    "        neg_count = n - pos_count\n",
    "        \n",
    "        # Effective number of samples (from Class-Balanced Loss paper)\n",
    "        eff_pos = (1 - beta**pos_count) / (1 - beta) if pos_count > 0 else 0\n",
    "        eff_neg = (1 - beta**neg_count) / (1 - beta) if neg_count > 0 else 0\n",
    "        \n",
    "        weight = eff_neg / (eff_pos + 1e-8)  # Avoid division by zero\n",
    "        weights.append(min(weight, 10.0))  # Cap extreme weights\n",
    "    \n",
    "    return torch.tensor(weights).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5373732a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:38.158266Z",
     "iopub.status.busy": "2025-10-04T10:40:38.158085Z",
     "iopub.status.idle": "2025-10-04T10:40:38.163146Z",
     "shell.execute_reply": "2025-10-04T10:40:38.162653Z"
    },
    "papermill": {
     "duration": 0.013642,
     "end_time": "2025-10-04T10:40:38.164186",
     "exception": false,
     "start_time": "2025-10-04T10:40:38.150544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.9997, device=None):\n",
    "        # copy *unwrapped* module (so EMA keys match saved/loaded model keys)\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        self.ema = copy.deepcopy(get_model(model)).eval()\n",
    "        if device is not None:\n",
    "            self.ema.to(device)\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        src_state = get_model(model).state_dict()\n",
    "        with torch.no_grad():\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                model_v = src_state[k].detach().to(v.device)\n",
    "                v.copy_(v * self.decay + (1.0 - self.decay) * model_v)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.ema.state_dict()\n",
    "\n",
    "\n",
    "def get_model(module):\n",
    "    \"\"\"Return the underlying model (unwrap DataParallel / DDP).\"\"\"\n",
    "    return module.module if hasattr(module, \"module\") else module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7bcedcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:40:38.179408Z",
     "iopub.status.busy": "2025-10-04T10:40:38.179228Z",
     "iopub.status.idle": "2025-10-04T16:27:08.251831Z",
     "shell.execute_reply": "2025-10-04T16:27:08.250911Z"
    },
    "papermill": {
     "duration": 20790.081636,
     "end_time": "2025-10-04T16:27:08.253075",
     "exception": false,
     "start_time": "2025-10-04T10:40:38.171439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "⚡ Using 2 GPUs\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 3391/3391 [57:22<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] TrainLoss=0.1086 \n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train]: 100%|██████████| 3391/3391 [57:39<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] TrainLoss=0.0982 \n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train]: 100%|██████████| 3391/3391 [57:44<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] TrainLoss=0.0947 \n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train]: 100%|██████████| 3391/3391 [57:48<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] TrainLoss=0.0915 \n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train]: 100%|██████████| 3391/3391 [57:53<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] TrainLoss=0.0879 \n",
      "✅ Saved new best model\n",
      "Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 3391/3391 [57:54<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] TrainLoss=0.0831 \n",
      "✅ Saved new best model\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=6\n",
    "\n",
    "# -----------------------------\n",
    "# Run Training\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "df_all = pd.read_csv('/kaggle/input/grand-xray-slam-division-b/train2.csv')\n",
    "df_all = df_all[~df_all['Image_name'].isin([\n",
    "    '00043046_001_001.jpg',\n",
    "    '00052495_001_001.jpg',\n",
    "    '00056890_001_001.jpg'\n",
    "])]\n",
    "\n",
    "label_cols = [\n",
    "            \"Atelectasis\",\n",
    "            \"Cardiomegaly\",\n",
    "            \"Consolidation\",\n",
    "            \"Edema\",\n",
    "            \"Enlarged Cardiomediastinum\",\n",
    "            \"Fracture\",\n",
    "            \"Lung Lesion\",\n",
    "            \"Lung Opacity\",\n",
    "            \"No Finding\",\n",
    "            \"Pleural Effusion\",\n",
    "            \"Pleural Other\",\n",
    "            \"Pneumonia\",\n",
    "            \"Pneumothorax\",\n",
    "            \"Support Devices\",\n",
    "        ]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create datasets\n",
    "train_df = df_all.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Model: EVAX base\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EVAX_Model(unfreeze_last_n_blocks=6)\n",
    "#model = model.to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Loss & Optimizer\n",
    "# -----------------------------\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "pos_weights = calculate_effective_weights(train_df, label_cols, beta=0.9999)\n",
    "criterion = WeightedFocalBCELoss(alpha=0.25, gamma=2.0, pos_weights=pos_weights.to(DEVICE))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': model.backbone.patch_embed.parameters(), 'lr': 1e-6, 'weight_decay': 1e-6},  \n",
    "        {'params': model.backbone.rope.parameters(), 'lr': 1e-6, 'weight_decay': 1e-6},  \n",
    "        {'params': model.backbone.blocks[0:4].parameters(), 'lr': 1e-6, 'weight_decay': 1e-4},  \n",
    "        {'params': model.backbone.blocks[4:6].parameters(), 'lr': 5e-6, 'weight_decay': 1e-4},  \n",
    "        {'params': model.backbone.blocks[6:8].parameters(), 'lr': 1e-5, 'weight_decay': 1e-4},  \n",
    "        {'params': model.backbone.blocks[8:10].parameters(), 'lr': 3e-5, 'weight_decay': 1e-4},  \n",
    "        {'params': model.backbone.blocks[10:].parameters(), 'lr': 5e-5, 'weight_decay': 1e-4},  \n",
    "        {'params': model.backbone.fc_norm.parameters(), 'lr': 5e-5, 'weight_decay': 1e-4},  \n",
    "        {'params': model.backbone.head.parameters(), 'lr': 8e-5, 'weight_decay': 0},      \n",
    "    ]\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"⚡ Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)  # wrap for multi-GPU\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "#EMA\n",
    "ema = ModelEMA(model, decay=0.9997)\n",
    "\n",
    "# Dataset\n",
    "train_dataset = ChestXrayDataset(\n",
    "    df=train_df,\n",
    "    img_dir=\"/kaggle/input/600p-div-b-data/train2_resized\",\n",
    "    transform=train_tfms\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,   \n",
    "    #num_workers=4,   \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, ema)\n",
    "    #val_loss, val_auc, per_label_auc = validate(ema.ema, val_loader, criterion)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] TrainLoss={train_loss:.4f} \")\n",
    "    # print(f\"[Epoch {epoch+1}] TrainLoss={train_loss:.4f} | EMA ValLoss={val_loss:.4f} | AUC={val_auc:.4f}\")\n",
    "    # print(\"Per-label AUROC:\", {k: f\"{v:.3f}\" for k, v in per_label_auc.items()})\n",
    "\n",
    "    if epoch > 3:\n",
    "        #best_auc = val_auc\n",
    "        save_path = f\"best_model_f{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"model_state_dict\": get_model(model).state_dict(),   # unwrap\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, save_path)\n",
    "    \n",
    "        # also save EMA\n",
    "        if ema is not None:\n",
    "            torch.save({\n",
    "                \"epoch\": epoch+1,\n",
    "                \"ema_state_dict\": ema.state_dict(),\n",
    "            }, f\"ema_model_f{epoch+1}.pth\")\n",
    "    \n",
    "        print(f\"✅ Saved new best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27835b27",
   "metadata": {
    "papermill": {
     "duration": 0.891563,
     "end_time": "2025-10-04T16:27:09.938044",
     "exception": false,
     "start_time": "2025-10-04T16:27:09.046481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13471427,
     "sourceId": 113002,
     "sourceType": "competition"
    },
    {
     "datasetId": 8304099,
     "sourceId": 13109240,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20853.50462,
   "end_time": "2025-10-04T16:27:14.175890",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-04T10:39:40.671270",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
